---
layout: default
---

# Chair of Data Science at JMU Wuerzburg

This page is an overview of research projects by the [Data Science chair](https://www.informatik.uni-wuerzburg.de/datascience/home/) at the Julius-Maximilians-University Wuerzburg.
Our research topics include Natural Language Processing, Recommender Systems and Deep Learning for Dynamical Systems.

Our organization is currently present at:
* Github: [LSX-UniWue](https://www.github.com/LSX-UniWue)
* Twitter: [datascience_jmu](https://twitter.com/datascience_jmu)

## Projects

### BibSonomy
[BibSonomy](https://www.bibsonomy.org/) is a social bookmarking system as well as the underlying technical platform.
BibSonomy is run and developed by the Data Science Chair in cooperation with the [KDE group](https://www.kde.cs.uni-kassel.de/) of the University of Kassel, and the [L3S Research Center](https://www.l3s.de), Germany.
The source code is available [here](https://bitbucket.org/bibsonomy/bibsonomy).

### DenseWeight
[DenseWeight](https://www.github.com/SteiMi/denseweight) is a python package that provides a method for dealing with imbalanced regression tasks.
The corresponding paper "Density-based weighting for imbalanced regression" is available [here](https://dx.doi.org/10.1007/s10994-021-06023-5).

### Emote-Controlled
[Code and data](https://github.com/LSX-UniWue/emote-controlled) for "Emote-Controlled: Obtaining Implicit Viewer Feedback through Emote based Sentiment Analysis on Comments of Popular Twitch.tv Channels"

### Where To Submit?
The [code and data](https://github.com/LSX-UniWue/wts) of our paper "Where to Submit? Helping Researchers to Choose the Right Venue". An interactive web demo can be found under [wheretosubmit.ml](https://wheretosubmit.ml).

### SimLoss
A [PyTorch implementation](https://github.com/LSX-UniWue/SimLoss) of our proposed loss function from the paper "SimLoss: Class Similarities in Cross Entropy"

### Do Different Deep Metric Learning Losses Lead to Similar Learned Features?
[Code and data](https://github.com/LSX-UniWue/DML-analysis) for our ICCV 2021 paper "Do Different Deep Metric Learning Losses Lead to Similar Learned Features?"
